[{"authors":["songyangzhang"],"categories":null,"content":"I\u0026rsquo;m a first-year PhD student at ShanghaiTech PLUS Group, advised by Xuming He. I received my undergraduate degree from Beihang University, working with Mai Xu. I have also spent time at research labs of TuSimple Inc, Megvii and Tencent Youtu.\n","date":1595548800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1595548800,"objectID":"ff95ff3f9b4ad8ebc440ff2eafb1f390","permalink":"http://syzhang.me/author/songyang-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/songyang-zhang/","section":"authors","summary":"I\u0026rsquo;m a first-year PhD student at ShanghaiTech PLUS Group, advised by Xuming He. I received my undergraduate degree from Beihang University, working with Mai Xu. I have also spent time at research labs of TuSimple Inc, Megvii and Tencent Youtu.","tags":null,"title":"Songyang Zhang","type":"authors"},{"authors":["Xi Chen","Songyang Zhang","Dandan Song","Peng Ouyang","Shouyi Yin"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595548800,"objectID":"7a991a474cfd5f1197e6fc8af23dc7c7","permalink":"http://syzhang.me/publication/year2020/conf_interspeech2020_sbdt/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/publication/year2020/conf_interspeech2020_sbdt/","section":"publication","summary":"Attention-based models have made tremendous progress on end-to-end automatic speech recognition(ASR) recently. However, the conventional transformer-based approaches usually generate the recognition sequence token by token from left to right, leaving the right-to-left contexts unexploited. In this work, we introduce a synchronous bidirectional speech transformer to utilize the different directional contexts simultaneously. Speciﬁcally, the outputs of our proposed transformer include a left-to-right target, and a right-to-left target. In inference stage, we use the introduced bidirectional beam search method, which can not only generate left-to-right candidates but also generate right-to-left candidates, and determine the best hypothesis sentence by scores. To demonstrate our proposed speech transformer with a bidirectional decoder(STBD), we conduct extensive experiments on the AISHELL-1 dataset. The results of experiments show that STBD achieves a 3.6% relative CER reduction(CERR) over the unidirectional speech transformer baseline, and the strongest model in this paper called STBD-Big model can achieve 6.64% CER on the test set, without language model rescoring and any extra data augmentation strategies.","tags":["Speech Transformer","Speech Recognition"],"title":"Transformer with Bidirectional Decoder for Speech Recognition","type":"publication"},{"authors":["Yongfei Liu","Xiangyi Zhang","Songyang Zhang","Xuming He"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1593734400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593734400,"objectID":"d1e7d4b86e4749da51a7af140cb20b9d","permalink":"http://syzhang.me/publication/year2020/conf_eccv2020_ppnet/","publishdate":"2020-07-11T00:00:00Z","relpermalink":"/publication/year2020/conf_eccv2020_ppnet/","section":"publication","summary":"Few-shot semantic segmentation aims to learn to segment new object classes with only a few annotated examples, which has a wide range of real-world applications. Most existing methods either focus on the restrictive setting of one-way few-shot segmentation or suffer from incomplete coverage of object regions. In this paper, we propose a novel few-shot semantic segmentation framework based on the prototype representation. Our key idea is to decompose the holistic class representation into a set of part-aware prototypes, capable of capturing diverse and fine-grained object features. In addition, we propose to leverage unlabeled data to enrich our part-aware prototypes, resulting in better modeling of intra-class variations of semantic objects. We develop a novel graph neural network model to generate and enhance the proposed part-aware prototypes based on labeled and unlabeled images. Extensive experimental evaluations on two benchmarks show that our method outperforms the prior art with a sizable margin.","tags":["Few-shot Segmentation","Graph Neural Network"],"title":"Part-aware prototype Network for Few-shot Semantic Segmentation","type":"publication"},{"authors":["Songyang Zhang"],"categories":[],"content":"最近在做Research Project的时候，发现有些小工具很好用，记录在此。\n1. 准确的FLOPS 计算 网上开源的很多计算flops的工具只支持计算PyTorch内置层的flops,不能有效计算出自定义操作的flops。Facebook日前开源了一个面向PyTorch的CV工具包，内置了​flops_count​函数，支持细粒度的flops计算，包括​torch.einsum​，​torch.bmm​等操作均可计算。同时还支持自定义一个operation白名单，用来控制计算那些操作的FLOPS。\n Example Link\n2. 参数量计算 这个比较常用了，记录在此，方便查阅\ndef params_count(model): \u0026quot;\u0026quot;\u0026quot; Compute the number of parameters. Args: model (model): model to count the number of parameters. \u0026quot;\u0026quot;\u0026quot; return np.sum([p.numel() for p in model.parameters()]).item()  3. 画出计算图 PyTorch 1.4版本内置了tensorboard，支持​add_graph ​，可以将我们定义的模型的计算图可视化出来，可以直观的看到每一层的size，和数据流向，为模型调试和验证提供了很好的帮助\n参见文档: https://pytorch.org/docs/stable/tensorboard.html\n4. 相关性分析 TensorFlow里有一个很好用的高维特征分析工具Projector, 内置TSNE和PCA。最新的PyTorch也加入了这个支持。可以先去http://projector.tensorflow.org/ 体验一下online版的。\n这个功能支持2D/3D的分析，同时可以交互式的点选，十分酷炫好用。\nimport torch from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter(\u0026quot;./visualization\u0026quot;) cat_features = np.loadtxt(\u0026quot;category_features.txt\u0026quot;) cat_labels = torch.load(\u0026quot;category_labels.txt\u0026quot;) writer.add_embedding(cat_features, cat_labels) writer.close()  ","date":1583884800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583884800,"objectID":"8ba5b31ea14059e3a0cf9677370248ab","permalink":"http://syzhang.me/post/zhihu/pytorch_tips_1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/zhihu/pytorch_tips_1/","section":"post","summary":"PyTorch Tips(FLOPs计算/参数量/计算图可视化/相关性分析)","tags":[],"title":"PyTorch Tips(FLOPs计算/参数量/计算图可视化/相关性分析)","type":"post"},{"authors":["Shuaiyi Huang","Qiuyue Wang","Songyang Zhang","Shipeng Yan","Xuming He"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   --  Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Huang_Dynamic_Context_Correspondence_ICCV_2019_supplemental.pdf). -- ","date":1563840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563840000,"objectID":"170eb2a0dffc41aad714aa1b7083dd48","permalink":"http://syzhang.me/publication/year2019/conf_iccv2019_dccnet/","publishdate":"2019-07-23T00:00:00Z","relpermalink":"/publication/year2019/conf_iccv2019_dccnet/","section":"publication","summary":"We instantiate our strategy by designing an end-to-end learnable deep network, named as Dynamic Context Correspondence Network (DCCNet), for the semantic correspondence problem.","tags":["Semantic Correspondence","Spatial Context"],"title":"Dynamic Context Correspondence Network for Semantic Alignment","type":"publication"},{"authors":["Songyang Zhang","Shipeng Yan","Xuming He"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   --  Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   -- ","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"3e09c7f1e0656fb91e8ab8118501dd12","permalink":"http://syzhang.me/publication/year2019/conf_icml2019_latentgnn/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/year2019/conf_icml2019_latentgnn/","section":"publication","summary":"Capturing long-range dependencies in feature representations is crucial for many visual recognition tasks. Despite recent successes of deep convolutional networks, it remains challenging to model non-local context relations between visual features. A promising strategy is to model the feature context by a fully-connected graph neural network(GNN), which augments traditional convolutional features with an estimated non-local context representation. However, most GNN-based approaches require computing a dense graph affinity matrix and hence have difficulty in scaling up to tackle complex real-world visual problems. In this work, we propose an efficient and yet flexible non-local relation representation based on a novel class of graph neural networks. Our key idea is to introduce a latent space to reduce the complexity of graph, which allows us to use a low-rank representation for the graph affinity matrix and to achieve a linear complexity in computation. Extensive experimental evaluations on three major visual recognition tasks show that our method outperforms the prior works with a large margin while maintaining a low computation cost.","tags":["Object Detection","Graph Neural Network","Attention"],"title":"LatentGNN: Learning Efficient Non-local Relations for Visual Recognition","type":"publication"},{"authors":["Shipeng Yan","Songyang Zhang","Xuming He"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   --  Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"94e1b8d1f234ad93cae34d91cda045ee","permalink":"http://syzhang.me/publication/year2019/conf_aaai2019_dualattention/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/year2019/conf_aaai2019_dualattention/","section":"publication","summary":"Despite recent success of deep neural networks, it remains challenging to efficiently learn new visual concepts from limited training data. To address this problem, a prevailing strategy is to build a meta-learner that learns prior knowledge on learning from a small set of annotated data. However, most of existing meta-learning approaches rely on a global representation of images and a meta-learner with complex model structures, which are sensitive to background clutter and difficult to interpret. We propose a novel meta-learning method for few-shot classification based on two simple attention mechanisms`:` one is a spatial attention to localize relevant object regions and the other is a task attention to select similar training data for label prediction. We implement our method via a dual-attention network and design a semantic-aware meta-learning loss to train the meta-learner network in an end-to-end manner. We validate our model on three few-shot image classification datasets with extensive ablative study, and our approach shows competitive performances over these datasets with fewer parameters. For facilitating the future research, code and data split are available`:`https://github.com/tonysy/STANet-PyTorch","tags":["Few-shot Learning","Meta Learning","Attention"],"title":"A Dual Attention Network With Semantic Embedding for Few-shot Learning","type":"publication"},{"authors":["Yuhang Song","Mai Xu","Songyang Zhang","Liangyu Huo"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   --  Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"2d118e09dbd770ebb00c5e99685e9ccf","permalink":"http://syzhang.me/publication/year2017/arxiv_nipsw2017_gtn/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/publication/year2017/arxiv_nipsw2017_gtn/","section":"publication","summary":"Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by incorporating deep neural networks in learning representations from the input to RL. However, the conventional deep neural network architecture is limited in learning representations for multi-task RL (MT-RL), as multiple tasks can refer to different kinds of representations. In this paper, we thus propose a novel deep neural network architecture, namely generalization tower network (GTN), which can achieve MT-RL within a single learned model. Specifically, the architecture of GTN is composed of both horizontal and vertical streams. In our GTN architecture, horizontal streams are used to learn representation shared in similar tasks. In contrast, the vertical streams are introduced to be more suitable for handling diverse tasks, which encodes hierarchical shared knowledge of these tasks. The effectiveness of the introduced vertical stream is validated by experimental results. Experimental results further verify that our GTN architecture is able to advance the state-of-the-art MT-RL, via being tested on 51 Atari games.","tags":["Reinforcement Learning"],"title":"Generalization Tower Network: A Novel Deep Neural Network Architecture for Multi-Task Learning","type":"publication"},{"authors":["Yufan Liu","Songyang Zhang","Mai Xu","Xuming He"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   --  Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"0820cf617a67dcd41a8665caa7c3f337","permalink":"http://syzhang.me/publication/year2017/conf_cvpr2017_salient/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/publication/year2017/conf_cvpr2017_salient/","section":"publication","summary":"Although the recent success of convolutional neural network (CNN) advances state-of-the-art saliency prediction in static images, few work has addressed the problem of predicting attention in videos. On the other hand, we find that the attention of different subjects consistently focuses on a single face in each frame of videos involving multiple faces. Therefore, we propose in this paper a novel deep learning (DL) based method to predict salient face in multiple-face videos, which is capable of learning features and transition of salient faces across video frames. In particular, we first learn a CNN for each frame to locate salient face. Taking CNN features as input, we develop a multiple-stream long short-term memory (M-LSTM) network to predict the temporal transition of salient faces in video sequences. To evaluate our DL-based method, we build a new eye-tracking database of multiple-face videos. The experimental results show that our method outperforms the prior state-of-the-art methods in predicting visual attention on faces in multipleface videos","tags":["Saliency"],"title":"Predicting Salient Face in Multiple-face Videos","type":"publication"}]