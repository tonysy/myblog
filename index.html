<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<!-- saved from url=(0041)http://www.zhangsongyang.com/#workshops -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" class="eye-protector-processed" style="background-color: rgba(0, 0, 0, 0);"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">

<link rel="stylesheet" href="./homepage_files/jemdoc.css" type="text/css">
<title>SONGYANG, ZHANG</title>
</head>
<body data-new-gr-c-s-check-loaded="14.1024.0" data-gr-ext-installed="" class="eye-protector-processed" style="background-color: rgb(255, 255, 255); transition: background-color 0.3s ease 0s;">

<div class="menu"> <a href="http://www.zhangsongyang.com/#home">Home</a> 
<a href="http://www.zhangsongyang.com/#biography">Biography</a> 
<a href="http://www.zhangsongyang.com/#news">News</a> 
  <a href="http://www.zhangsongyang.com/#publications">Publications</a> 
<!-- <a href="http://www.zhangsongyang.com/#patents">Patents</a>  -->
<a href="http://www.zhangsongyang.com/#awards">Awards</a> 
<!-- <a href="http://www.zhangsongyang.com/#tutorials">Tutorials</a>  -->
<a href="http://www.zhangsongyang.com/#talks">Talks</a>
<a href="http://www.zhangsongyang.com/#services">Services</a> 
<!-- <a href="http://www.zhangsongyang.com/#teaching">Teaching</a> -->
</div>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container eye-protector-processed" style="transition: background-color 0.3s ease 0s; background-color: rgb(255, 255, 255);"> 
<div id="toptitle" class="eye-protector-processed" style="border-bottom-color: rgb(128, 128, 128);">
<h1>Songyang, Zhang</h1>
</div>

<table class="imgtable"><tbody><tr><td>
<a href="http://www.zhangsongyang.com/"><img src="./homepage_files/images/songyang.png" alt="" height="200px"></a>&nbsp;</td>
<td align="left"><p><a href="http://www.zhangsongyang.com"><font size="4">Songyang, Zhang</font></a><br>
  <img src="./homepage_files/images/name.PNG" alt="" height="24px"><br>

 <i>Postdoctoral Research Fellow</i>


 <br><br>
 <a href="https://openmmlab.com/">OpenMMLab</a><br>
 Shanghai AI Laboratory <br>
 Shanghai, China<br>
 <br>
 
Email: <i>sy.zhangbuaa<img src="./homepage_files/images/at_logo.png" alt="" height="11px">gmail.com</i><br>
<br>
[<a href="https://scholar.google.com/citations?user=8XQPi7YAAAAJ&hl=en" target="_blank">Google Scholar</a>]
</p>
</td></tr></tbody></table>

<a id="biography" class="achor"></a>
<h2 class="eye-protector-processed" style="border-bottom-color: rgb(170, 170, 170);">Biography</h2>
<p style="text-align:justify"> I am currently a Postdoctoral Research Fellow at Shanghai AI Laboratory, worked with  <a href="https://chenkai.site/">Dr. Kai Chen</a> and <a href="http://dahua.site/">Prof. Dahua Lin</a>. </p>
  
<p style="text-align:justify">
  I obtained my Ph.D. in Computer Science at the University of Chinese Academy of Science, in the joint program at <a href="http://plus.sist.shanghaitech.edu.cn/">PLUS Lab</a>, <a href="http://sist.shanghaitech.edu.cn/">ShanghaiTech University</a>, China, under the supervision of <a href="https://xmhe.bitbucket.io/">Prof. Dr. Xuming He</a>. My research interests include visual recognition with limited data and model architecture design. I obtained the B.Sc. degree in 2017 at the <a href="http://www.buaamc2.net/">MC^2 Lab</a>, <a href="http://ev.buaa.edu.cn/">Beihang University</a>, P.R. China, under the supervision of <a href="http://www.buaamc2.net/">Prof. Dr. Mai Xu</a>. I also worked as a Research Intern in TuSimple, Nullmax AI, Tencent YouTu Lab, and Megvii Research.
</p>

<p style="text-align:justify"><font color="red">Our team focus on foundation model of computer vision at OpenMMLab. Open positions at OpenMMLab include full-time researchers/engineers and interns. If you are interested in joining OpenMMLab, please feel free to contact me through the email. Research directions include: model architecture design, self/semi/weak-supervised learning, 2d/3d detection, segmentation, etc. 
</font></p>



<!-- News Section-->
<a id="news" class="achor"></a>
<h2 class="eye-protector-processed" style="border-bottom-color: rgb(170, 170, 170);">News</h2>
<ul>
  <li>Two papers on <b>Semantic Correspondence</b> and <b>Video Quality Assessment</b> are accepted by <b>ECCV 2022</b>.
  <li>One paper on <b>Few-shot Video Segmentation</b></font> is accepted by <b>ICIP 2022</b>.
     <!-- [<a href="">Paper</a>] -->
    </li>
  <li>I have joint <b>Shanghai AI Laboratory</b> as a postdoctoral research fellow.</li>
  <li>One paper on <b>Transformer-based Scene Graph Generation</b></font> is accepted by <b>CVPR 2022</b>. [<a href="https://arxiv.org/abs/2112.12970">Paper</a>]</li>

  <li>We won the <font color="red"><b>1st Place</b></font> on <b>System Design Contest(SDC) @ DAC 2021</b>. [<a href="https://dac-sdc-2021.groups.et.byu.net/doku.php?id=results">Webpage</a>]</li>


  <li>We won the <font color="red"><b>2nd Place</b></font> on <b>LVIS Challenge @ ICCV 2021</b>. [<a href="https://www.lvisdataset.org/challenge_2021">Webpage</a>]</li>

<li>I have been selected to  <b>ICCV 2021 Doctoral Consortium</b>.</li>

<li>One paper on <b>Vision Transformer</b> is accepted to <b>NeurIPS</b> 2021. [<a href="https://papers.nips.cc/paper/2021/hash/2d969e2cee8cfa07ce7ca0bb13c7a36d-Abstract.html">Paper</a>]</li>

<li>One paper on <b>Incremental Semantic Segmenntation</b> is accepted to <b>ACM MM</b> 2021. [<a href="https://arxiv.org/abs/2108.03613">Paper</a>]</li>

<li>We won the <font color="red"><b>1st Place</b></font> on Streaming Perception Challenges (Workshop on Autonomous Driving at CVPR 2021). [<a href="https://arxiv.org/abs/2108.04230">Report</a>]</li>

<li>One paper on <b>Few-shot Video Classification</b> is accepted to <b>IJCAI</b> 2021. [<a href="https://arxiv.org/abs/2105.04823">Paper</a>]</li>

<li>One paper on <b>Scene Graph Generation</b> is accepted to <b>CVPR</b> 2021. [<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Li_Bipartite_Graph_Network_With_Adaptive_Message_Passing_for_Unbiased_Scene_CVPR_2021_paper.html">Paper</a>]</li>

<li>One paper on <b>Long-tail Visual Recognition</b> is accepted to <b>CVPR</b> 2021. [<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Distribution_Alignment_A_Unified_Framework_for_Long-Tail_Visual_Recognition_CVPR_2021_paper.html">Paper</a>]</li>

<!-- <li>One paper on <b>Speech Recognition with Transformer</b> is accepted to <b>InterSpeech</b> 2020. [<a href="https://arxiv.org/abs/2008.04481">Paper</a>]</li> -->


<!-- <li>One paper on <b>Few-shot Semantic Segmenntation</b> is accepted to <b>ECCV</b> 2020. [<a href="https://arxiv.org/abs/2007.06309">Paper</a>]</li> -->

<!-- <li>I won the <b><font color="red">National Scholarship for Master Student</font></b>, 2019. [<a href="">Certificate</a>]</li> -->


<!-- <li>One paper on <b>Efficient Non-local Relation Modeling</b> is accepted to <b>ICML</b> 2019. [<a href="http://proceedings.mlr.press/v97/zhang19f.html">Paper</a>]</li> -->



<!-- <li>I serve as a Guest Speaker for the <b>Learning for Visual Data Compression Tutorial</b> at CVPR 2021. [<a href="https://guolu-home.github.io/cvpr21-tutorial">Slides</a>]</li>
<li>We organize the <a href="https://github.com/RenYang-home/NTIRE21_VEnh">Video Enhancement Challenge</a> at the <a href="https://data.vision.ee.ethz.ch/cvl/ntire21/">NTIRE workshop</a> in CVPR 2021. [<a href="https://arxiv.org/abs/2104.10782">Dataset</a>] [<a href="https://arxiv.org/abs/2104.10781">Methods</a>]</li>
<li>I am selected in the Finalist of the <a href="https://www.qualcomm.com/research/research/university-relations/innovation-fellowship/2021-europe">Qualcomm Innovation Fellowship Europe 2021</a>.</li>
<li>One paper is accepted to <b>CVPR</b> 2021 <font color="red"><b>(Oral)</b></font>. [<a href="http://buaamc2.net/pdf/cvpr21hesic.pdf">Paper</a>] [<a href="https://github.com/ywz978020607/HESIC">Code</a>]</li>
<li>Our <b>US Patent</b> for multi-frame video enhancement has been <b>Granted</b>. [<a href="https://patents.google.com/patent/US20200404340A1/en">Google Patent</a>]
</li><li>I serve as a <b>Senior Program Committee (SPC)</b> member for IJCAI 2021. [<a href="http://renyang-home.github.io/papers/IJCAI_SPC.pdf">Certificate</a>]
</li><li>Talk on Learning-based Video Compression, Dec. 23, 2020 [<a href="http://renyang-home.github.io/files/zhidongxi_1223.pdf">Slide</a>] [<a href="https://apposcmf8kb5033.h5.xiaoeknow.com/v1/course/alive/l_5fd8a285e4b0231ba88ce9fc?app_id=appoSCMf8kb5033&amp;is_redirect=1&amp;scene=%E5%88%86%E4%BA%AB&amp;share_type=5&amp;share_user_id=u_5eb92d577fec0_f0a8lSsIPJ&amp;type=2">Video record</a> (in Chinese)]</li>
<li>One paper is accepted to IEEE Journal of Selected Topics in Signal Processing (<b>J-STSP</b>). [<a href="https://ieeexplore.ieee.org/abstract/document/9288876">Paper</a>] [<a href="https://github.com/RenYang-home/RLVC">Code</a>]</li>
<li>We deliver a <b>Tutorial</b> on Learned Image and Video Compression at IEEE VCIP 2020. [<a href="https://ieeexplore.ieee.org/abstract/document/9301828">Abstract</a>] [<a href="http://renyang-home.github.io/papers/VCIP_Tutorial.pdf">Slide</a>] [<a href="https://www.polybox.ethz.ch/index.php/s/PSmRYBTyvljzkbm">Video record</a>]</li>
<li>I supervise a <b>Master's Semester Thesis</b> on learned image compression at ETH Zurich. [<a href="https://link.springer.com/chapter/10.1007/978-3-030-66823-5_12">Paper</a>]
</li><li>One paper is accepted to <b>CVPR</b> 2020. [<a href="http://arxiv.org/abs/2003.01966">Paper</a>] [<a href="https://github.com/RenYang-home/HLVC">Code</a>]</li>
<li>One paper (collaborated with <a href="https://jiaxinlu-home.github.io/">my wife</a>) is accepted to IEEE Trans. on Image Processing (<b>T-IP</b>). [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9025769">Paper</a>] [<a href="https://github.com/RenYang-home/Natural-Scene-Memorability">Code</a>] [<a href="https://github.com/JiaxinLu-home/Natural-Scene-Memorability-Dataset">Dataset</a>]</li>
<li>I won the <b>2019 Outstanding Master Thesis Award</b> of <a href="https://www.cie-info.org.cn/">Chinese Institute of Elecronics</a>. [<a href="https://www.cie.org.cn/site/content/3519.html">News</a>]</li>
<li>One paper is accepted to <b>ICCV</b> 2019 <font color="red"><b>(Oral)</b></font>. [<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Wavelet_Domain_Style_Transfer_for_an_Effective_Perception-Distortion_Tradeoff_in_ICCV_2019_paper.pdf">Paper</a>] [<a href="https://github.com/cindydeng1991/Wavelet-Domain-Style-Transfer-for-an-Effective-Perception-distortion-Tradeoff-in-Single-Image-Super-">Code</a>] </li>
<li>One paper is accepted to <b>IEEE T-PAMI</b>. [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8855019">Paper</a>] [<a href="https://github.com/RyanXingQL/MFQEv2.0">Code</a>] </li>
<li>I am the <b>Winner of <a href="https://threeminutethesis.uq.edu.au/">Three Minute Thesis Competition</a></b> at <b>IEEE ICME</b> 2019. [<a href="http://www.zhangsongyang.com/papers/ICMEaward.jpg">Certificate</a>]</li> -->

</ul> 

<!-- Publications Section-->
<a id="publications" class="achor"></a>
<h2 class="eye-protector-processed" style="border-bottom-color: rgb(170, 170, 170);">Publications</h2>

<table class="imgtable">

<tbody>
  <tr>
    <td><img class="proj_thumb" src="./homepage_files/images/publication/year2021/sgtr2021.png" alt="">&nbsp;</td>
    <td>
    <p class="pub_title">SGTR: End-to-end  Scene Graph Generation with Transformer</p>
    <p class="pub_author">Rongjie Li, <u><b>Songyang Zhang</b></u>,&nbsp;Xuming He.<br>
      IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.  <br> 
    [<a href="https://arxiv.org/abs/2112.12970">Paper</a>] 
    <!-- [<a href="https://github.com/SHTUPLUS/PySGG">Code</a>] -->
    </p> </td>
    </tr>

  <tr>
    <td><img class="proj_thumb" src="./homepage_files/images/publication/year2021/dge2021.png" alt="">&nbsp; </td>
    <td>
    <p class="pub_title"> Dynamic Grained Encoder for Vision Transformers 
    </p>
    <p class="pub_author"> Lin Song*, <u><b>Songyang Zhang*</b></u>, Songtao Liu, Zeming Li, Xuming He, Hongbin Sun, Jian Sun, Nanning Zheng,<br>
      (* means equal contribution)<br>
    Neural Information Processing Systems (<b>NeurIPS</b>), 2021. <br> 
    [<a href="https://papers.nips.cc/paper/2021/hash/2d969e2cee8cfa07ce7ca0bb13c7a36d-Abstract.html">Paper</a>]
    [<a href="https://github.com/StevenGrove/vtpack">Code</a>]
    </p> </td>
    </tr>
    
  <tr>

  <tr>
    <td><img class="proj_thumb" src="./homepage_files/images/publication/year2021/wad2021.jpg" alt="">&nbsp; </td>
    <td>
    <p class="pub_title"> Workshop on Autonomous Driving at CVPR 2021: Technical Report for Streaming Perception Challenge
    </p>
    <p class="pub_author"><u><b>Songyang Zhang*</b></u>, Lin Song*, Songtao Liu*, Zheng Ge, Zeming Li, Xuming He, Jian Sun.<br>
      (* means equal contribution)<br>
    Technical Report of the <font color="red"><b>1st Place</b></font> on Streaming Perception Challenges, 2021. <br> 
    [<a href="https://arxiv.org/abs/2108.04230">Paper</a>]
    </p> </td>
    </tr>
    
  <tr>
<td><img class="proj_thumb" src="./homepage_files/images/publication/year2021/acmmm2021.jpg" alt="">&nbsp; </td>
<td>
<p class="pub_title">An EM Framework for Online Incremental Learning of Semantic Segmentation</p>
<p class="pub_author">Shipeng Yan*, Jiale Zhou*, Jiangwei Xie, <u><b>Songyang Zhang</b></u>, Xuming He.<br>
(* means equal contribution)<br>
The 29th ACM International Conference on Multimedia (<b>ACM MM</b>), 2021. <br> 
[<a href="https://arxiv.org/abs/2108.03613">Paper</a>]
</p> </td>
</tr>

<tr>
<td><img class="proj_thumb" src="./homepage_files/images/publication/year2021/itanet.png" alt="">&nbsp; </td>
<td>
<p class="pub_title">Learning Implicit Temporal Alignment for Few-shot Video Classification</p>
<p class="pub_author"><u><b>Songyang Zhang*</b></u>,&nbsp;Jiale Zhou*,&nbsp;Xumieng He.<br>
  (* means equal contribution)<br>
International Joint Conferences on Artificial Intelligence, (<b>IJCAI</b>), 2021. <br> 
[<a href="https://arxiv.org/abs/2105.04823">Paper</a>] [<a href="https://github.com/tonysy/PyAction">Code</a>]
</p> </td>
</tr>

<tr>
<td><img class="proj_thumb" src="./homepage_files/images/publication/year2021/bgnn2021.png" alt="">&nbsp;</td>
<td>
<p class="pub_title">Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene Graph Generation</p>
<p class="pub_author">Rongjie Li, <u><b>Songyang Zhang</b></u>,&nbsp;Bo Wan,&nbsp;Xuming He.<br>
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.  <br> 
[<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Li_Bipartite_Graph_Network_With_Adaptive_Message_Passing_for_Unbiased_Scene_CVPR_2021_paper.html">Paper</a>] [<a href="https://github.com/SHTUPLUS/PySGG">Code</a>]
</p> </td>
</tr>

<tr>
<td><img class="proj_thumb" src="./homepage_files/images/publication/year2021/disalign2021.png" alt="">&nbsp;</td>
<td>
<p class="pub_title">Distribution Alignment: A Unified Framework for Long-tail Visual Recognition</p>
<p class="pub_author"><u><b>Songyang Zhang</b></u>, Zeming Li, Shipeng Yan, Xuming He, Jian Sun.<br>
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.  <br> 
[<a href="http://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Distribution_Alignment_A_Unified_Framework_for_Long-Tail_Visual_Recognition_CVPR_2021_paper.html">Paper</a>] [<a href="https://github.com/Megvii-BaseDetection/DisAlign">Code</a>]
</p> </td>
</tr>
  

<tr>
  <td><img class="proj_thumb" src="./homepage_files/images/publication/year2020/interspeech.jpg" alt="">&nbsp;</td>
  <td>
  <p class="pub_title">Transformer with Bidirectional Decoder for Speech Recognition  </p>
  <p class="pub_author">Xi Chen, <u><b>Songyang Zhang</b></u>, Dandan Song, Peng Ouyang, Shouyi Yin.<br>
    The Conference of the International Speech Communication Association , (<b>InterSpeech</b>), 2020. <br> 
  [<a href="https://arxiv.org/abs/2008.04481">Paper</a>]
  </p> </td>
  </tr>

  
<tr>
<td><img class="proj_thumb" src="./homepage_files/images/publication/year2020/ppnet.png" alt="">&nbsp;</td>
<td>
<p class="pub_title">Part-aware Prototype Network for Few-shot Semantic Segmentation</p>
<p class="pub_author">Yongfei Liu*, Xiangyi Zhang*, <u><b>Songyang Zhang</b></u>,&nbsp;Xuming He.<br>
  (* means equal contribution)<br>
  European Conference of Computer Vision, (<b>ECCV</b>), 2020. <br> 
[<a href="https://arxiv.org/abs/2007.06309">Paper</a>] [<a href="https://github.com/Xiangyi1996/PPNet-PyTorch">Code</a>]
</p> </td>
</tr>

<tr>
<td><img class="proj_thumb" src="./homepage_files/images/publication/year2019/latentgnn.png" alt="">&nbsp;</td>
<td>
<p class="pub_title">LatentGNN: Learning Efficient Non-local Relations for Visual Recognition</p>
<p class="pub_author"><u><b>Songyang Zhang</b></u>, Shipeng Yan, Xuming He.<br>
The 36th International Conference on Machine Learning, (<b>ICML</b>), 2019. <br> 
[<a href="http://proceedings.mlr.press/v97/zhang19f.html">Paper</a>] [<a href="https://github.com/latentgnn/LatentGNN-V1-PyTorch">Code</a>]
</p> </td>
</tr>

<tr>
<td><img class="proj_thumb" src="./homepage_files/images/publication/year2019/stanet.png" alt="">&nbsp;</td>
<td>
<p class="pub_title">A Dual Attention Network With Semantic Embedding for Few-shot Learning
</p>
<p class="pub_author">Shipeng Yan*,&nbsp;<u><b>Songyang Zhang*</b></u>,&nbsp;Xuming He.<br>
  (* means equal contribution)<br>
  Association for the Advancement of Artificial Intelligence, (<b>AAAI</b>), 2019. <br>
[<a href="https://xmhe.bitbucket.io/papers/stanet_aaai19.pdf">Paper</a>] [<a href="https://github.com/tonysy/STANet-PyTorch">Code</a>] </p> </td>
</tr>

<tr>
<td><img class="proj_thumb" src="./homepage_files/images/publication/year2019/dccnet.png" alt="">&nbsp;</td>
<td>
<p class="pub_title">Dynamic Context Correspondence Network for Semantic Alignment</p>
<p class="pub_author">Shuaiyi Huang, Qiuyue Wang, <u><b>Songyang Zhang</b></u>, Shipeng Yan, Xuming He.<br>
  International Conference on Computer Vision (<b>ICCV</b>), 2019.  <br>
[<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Dynamic_Context_Correspondence_Network_for_Semantic_Alignment_ICCV_2019_paper.pdf">Paper</a>] [<a href="https://github.com/ShuaiyiHuang/DCCNet">Code</a>] </p> </td>
</tr>

<tr>
<td><img class="proj_thumb" src="./homepage_files/images/publication/year2017/salient.png" alt="">&nbsp;</td>
<td>
<p class="pub_title">Predicting Salient Face in Multiple-face Videos</p>
<p class="pub_author">Yufan Liu,&nbsp;<u><b>Songyang Zhang</b></u>,&nbsp;Mai Xu, Xuming He.<br>
  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2017. <br>
[<a href="https://xmhe.bitbucket.io/papers/salientface_cvpr17.pdf">Paper</a>] </p> </td>
</tr>

</tbody></table>

<!-- Patents -->
<!-- <a id="patents" class="achor"></a>
<h2 class="eye-protector-processed" style="border-bottom-color: rgb(170, 170, 170);">Patents</h2>
<ul>
<li><b>Multi-frame quality enhancement method and device for lossy compressed video</b> <br>
  WO2019154152A1 [<a href="https://patents.google.com/patent/WO2019154152A1/en">Google Patent</a>] <br>
  US10965959B2, 2021 [<a href="https://patents.google.com/patent/US10965959B2/en">Google Patent</a>] [<a href="http://www.zhangsongyang.com/papers/patent_US.pdf">Certificate</a>]<br>
  CN108307193B, 2018 [<a href="https://patents.google.com/patent/CN108307193B/zh">Google Patent</a>] [<a href="http://www.zhangsongyang.com/papers/patent_MFQE.pdf">Certificate</a>] <br>
</li>
</ul>
<ul>
<li><b>Deep learning method-based block segmentation coding complexity optimization method and device</b> <br>
  WO2019179523A1 [<a href="https://patents.google.com/patent/WO2019179523A1/en">Google Patent</a>] <br>
  CN108495129B, 2019 [<a href="https://patents.google.com/patent/CN108495129B/zh">Google Patent</a>] [<a href="http://www.zhangsongyang.com/papers/patent_CTU.pdf">Certificate</a>] <br>
   </li>
</ul>
<ul>
<li><b>A CNN-based method for image and video enhancement</b><br>
  CN107481209B, 2020 [<a href="https://patents.google.com/patent/CN107481209A/zh">Google Patent</a>] [<a href="http://www.zhangsongyang.com/papers/patent_sfqe.pdf">Certificate</a>]<br>
  </li>
</ul>
<ul>
<li><b>A saliency-guided method for complexity control of HEVC decoding</b> <br>
  CN106210717B, 2017 [<a href="https://patents.google.com/patent/CN106210717A/en?oq=%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E8%A7%86%E9%A2%91%E6%98%BE%E8%91%97%E6%80%A7%E7%9A%84HEVC%E8%A7%A3%E7%A0%81%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95">Google Patent</a>] [<a href="http://www.zhangsongyang.com/RenYang-home.github.io/papers/patent_sgcc.pdf">Certificate</a>] <br>
  </li>
</ul> -->


<!-- Award -->
<a id="awards" class="achor"></a>
<h2 class="eye-protector-processed" style="border-bottom-color: rgb(170, 170, 170);">Awards and Honors</h2>
<ul>

  <li><b>The <font color="red">1st Place</font> on<a href="https://dac-sdc-2021.groups.et.byu.net/doku.php?id=results">System Design Challenge</a>, DAC 2021</b>. [<a href="http://www.zhangsongyang.com">Certificate</a>]</li>

  <li><b>The <font color="red">1st Place</font> on<a href="https://arxiv.org/abs/2108.04230"> Streaming Perception Challenge </a>, Workshop on Autonomous Driving at CVPR 2021</b>. [<a href="http://www.zhangsongyang.com">Certificate</a>]</li>

  <li><b>National Scholarship, P.R. China. (2019)</b>[<a href="http://www.zhangsongyang.com/">Certificate</a>]</li>

  <li><b>Outstanding Graduate Award, Beijing, P.R. China. (2017)</b> [<a href="http://www.zhangsongyang.com/">Certificate</a>]</li>

  <li><b>National <font color="red">1st Prize</font>, Challenge-Cup National College Student Business Plan Competition. (2016) </b>  [<a href="http://www.zhangsongyang.com/">Certificate</a>]</li>

<li><b>National <font color="red">1st Prize</font>, National Cryptology Contest for Undergraduate. (2015) </b>. [<a href="http://www.zhangsongyang.com/">Certificate</a>]</li>

<li><b>National <font color="red">2nd Prize</font>, National Electronic Design Contest for Undergraduate. (2015) </b>. [<a href="http://www.zhangsongyang.com/">Certificate</a>]</li>

</ul>



<a id="talks" class="achor"></a>
<h2 class="eye-protector-processed" style="border-bottom-color: rgb(170, 170, 170);">Talks</h2>
<ul>
<li><b>Invited Talk on Long-tail Visual Recognition,</b>  Tencent YouTu, Shanghai(Online), 2021. [<a href="http://www.zhangsongyang.com/homepage_files/files/longtail_cvpr_youtu.pdf">Slide</a>]</li>
<li><b>Invited Talk on Long-tail Visual Recognition,</b>  CUHK, Shenzhen(Online), 2021. [<a href="http://www.zhangruimao.site/seminar.html">Video</a>]</li>
<li><b>Invited Talk on Context Modeling in Visual Recognition</b>, MSRA, Beijing, 2019. [<a href="http://www.zhangsongyang.com/homepage_files/files/MSRA.pdf">Slide</a>]</li>
<!-- <li><b>Talk on Context Modeling in Visual Recognition</b>, MSRA, Beijing. [<a href="http://www.zhangsongyang.com/">Slide</a>]</li> -->
<!-- <li><b>Award of Excellence</b> at Microsoft Research. [<a href="http://www.zhangsongyang.com/papers/MSRaward.jpg">Certificate</a>]</li>
<li><b>TOP 10</b> Graduate Students Award, Beihang University. [<a href="http://www.zhangsongyang.com/papers/Top10.jpg">Certificate</a>]</li>
<li>National Scholarship, P.R. China. [<a href="http://www.zhangsongyang.com/papers/guojiang.jpg">Certificate</a>]</li> -->
</ul>

<!-- Tutorials -->
<!-- <a id="tutorials" class="anchor"></a>
<h2 class="eye-protector-processed" style="border-bottom-color: rgb(170, 170, 170);">Tutorials</h2> 
<ul>
<li>CVPR 2021: Learning for Visual Data Compression. [<a href="https://guolu-home.github.io/cvpr21-tutorial">Tutorial homepage</a>]
</li><li>IEEE VCIP 2020: Learned Image and Video Compression with Deep Neural Networks. [<a href="https://ieeexplore.ieee.org/abstract/document/9301828">Abstract</a>] [<a href="http://renyang-home.github.io/papers/VCIP_Tutorial.pdf">Slide</a>] [<a href="https://www.polybox.ethz.ch/index.php/s/PSmRYBTyvljzkbm">Video record</a>]</li>
</ul> -->

<!-- Workshops -->
<!-- <a id="workshops" class="anchor"></a>
<h2 class="eye-protector-processed" style="border-bottom-color: rgb(170, 170, 170);">Workshops</h2> 
<ul>
<li><a href="https://data.vision.ee.ethz.ch/cvl/ntire21/">New Trends in Image Restoration and Enhancement (NTIRE) workshop</a>, in conjunction with CVPR 2021.
  <ul>
    <li><a href="https://github.com/RenYang-home/NTIRE21_VEnh">NTIRE 2021 Challenge on Quality Enhancement of Compressed Video</a> [<a href="https://arxiv.org/abs/2104.10782">Dataset</a>] [<a href="https://arxiv.org/abs/2104.10781">Methods</a>]</li>
  </ul>
</li>
</ul> -->

<!-- Services -->
<a id="services" class="anchor"></a>
<h2 class="eye-protector-processed" style="border-bottom-color: rgb(170, 170, 170);">Services</h2>

<!-- <p>Senior Program Commitee (SPC) Member: </p>
<ul>
  <li>International Joint Conference on Artificial Intelligence (IJCAI 2021) [<a href="http://renyang-home.github.io/papers/IJCAI_SPC.pdf">Certificate</a>]</li>
</ul> -->

<p>Journal Reviewer: </p>
<ul>

<!-- <li>International Journal on Computer Vision (IJCV)</li> -->
<!-- <li>IEEE Transactions on Image Processing (T-IP)</li> -->
<!-- <li>IEEE Journal of Selected Topics in Signal Processing (J-STSP)</li> -->
<!-- <li>IEEE Transactions on Multimedia (T-MM)</li> -->
<li>IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT)</li>
<li>IEEE Transactions on Pattern Recognition and Machine Learning(TPAMI)</li>
<li>IET Computer Vision</li>
<!-- <li>IEEE Open Journal of Circuits and Systems (OJ-CAS)</li> -->
<!-- <li>IEEE Access</li> -->
<!-- <li>Signal Processing: Image Communication</li> -->
<li>Neurocomputing</li>

</ul>
<p>Conference Reviewer: </p>
<ul>
<li>International Conference on Learning Representations (ICLR 2022)</li>
<li>International Conference on Machine Learning (ICML 2020/2021)</li>
<li>Conference on Neural Information Processing Systems (NeurIPS 2020/2021)</li>
<li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2021)</li>
<li>IEEE International Conference on Computer Vision (ICCV 2021)</li>
<li>European Conference on Computer Vision (ECCV 2020)</li>
<li>AAAI Conference on Artificial Intelligence (AAAI 2020/2021/2022)</li>
<!-- <li>Asian Conference on Computer Vision (ACCV 2020)</li> -->
<!-- <li>IEEE Visual Communications and Image Processing (VCIP 2021) </li> -->
<!-- <li>IEEE/CIC International Conference on Communications in China (ICCC 2018)</li> -->
</ul>

<!-- Teaching -->
<!-- <a id="teaching" class="anchor"></a>
<h2 class="eye-protector-processed" style="border-bottom-color: rgb(170, 170, 170);">Teaching</h2> 
<ul>
<li>Supervisor: <a href="https://link.springer.com/chapter/10.1007/978-3-030-66823-5_12">Master's Semester Thesis</a> on learned image compression, ETH Zurich (Spring 2020) </li>
<li>Teaching Assistant: Digital Image Processing, Beihang University (Spring 2017) </li>
</ul> -->

<div id="footer" class="eye-protector-processed" style="border-color: rgb(192, 192, 192); color: rgb(192, 192, 192); transition: undefined 0s ease 0s;">
</div>

Research Collaborator: 
<a href="https://yanshipeng.com/">颜世鹏(Shipeng Yan)</a>,
<a href="https://www.zemingli.com/">黎泽明(Zeming Li)</a>,
<a href="https://sites.google.com/view/yongfei-liu">刘永飞(Yongfei Liu)</a>,<a href="https://shuaiyihuang.github.io/">黄帅一(Shuaiyi Huang)</a>,
</div>
<script
type="text/javascript"
src="//rf.revolvermaps.com/0/0/6.js?i=59tayzj4srf&amp;m=8&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=0"
async="async"
></script>
</div>
<!-- <div style="max-width: 341px; margin: 0px auto;"><div style="position: relative; padding-top: 100%;"><iframe style="background: transparent !important; position: absolute; left: 0px; top: 0px;" scrolling="no" frameborder="0" allowtransparency="true" width="100%" height="100%" src="./homepage_files/a2.html"></iframe></div></div><script type="text/javascript" src="./homepage_files/8.js" async="async"></script> -->

<!-- <div> -->
<!-- </div> -->
<script>mendeleyWebImporter = {
  downloadPdfs(t,e) { return this._call('downloadPdfs', [t,e]); },
  open() { return this._call('open', []); },
  setLoginToken(t) { return this._call('setLoginToken', [t]); },
  _call(methodName, methodArgs) {
    const id = Math.random();
    window.postMessage({ id, token: '0.18845877714678916', methodName, methodArgs }, 'http://www.zhangsongyang.com');
    return new Promise(resolve => {
      const listener = window.addEventListener('message', event => {
        const data = event.data;
        if (typeof data !== 'object' || !('result' in data) || data.id !== id) return;
        window.removeEventListener('message', listener);
        resolve(data.result);
      });
    });
  }
};</script>
</body>
<grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>

</html>